



%%writefile pyspark_bq_text_cleaning.py



From pyspark.sql import SparkSession

From pyspark.sql.functions import col, udf

From pyspark.sql.types import StringType

Import nltk

From nltk.stem import WordNetLemmatizer, PorterStemmer

From nltk.corpus import stopwords

Import re



# Download NLTK resources

Nltk.download(‘wordnet’)

Nltk.download(‘stopwords’)



# Initialize lemmatizer and stemmer

Lemmatizer = WordNetLemmatizer()

Stemmer = PorterStemmer()

Stop_words = set(stopwords.words(‘english’))



# Define text cleaning, lemmatization, and stemming functions

Def clean_text(text):

    Text = re.sub(r’\W’, ‘ ‘, text)  # Remove special characters

    Text = re.sub(r’\s+’, ‘ ‘, text)  # Remove extra spaces

    Text = re.sub(r’\d’, ‘’, text)  # Remove digits

    Text = text.strip().lower()  # Strip spaces and convert to lowercase

    Return text



Def lemmatize_text(text):

    Return ‘ ‘.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])



Def stem_text(text):

    Return ‘ ‘.join([stemmer.stem(word) for word in text.split() if word not in stop_words])



# Wrap functions as UDFs

Clean_text_udf = udf(clean_text, StringType())

Lemmatize_text_udf = udf(lemmatize_text, StringType())

Stem_text_udf = udf(stem_text, StringType())



# Initialize SparkSession

Spark = SparkSession.builder \

    .appName(“BigQueryTextProcessing”) \

    .config(“spark.jars”, “path/to/spark-bigquery-connector.jar”) \

    .getOrCreate()



# Read data from BigQuery using SQL query

Project_id = “your-project-id”

Dataset = “your-dataset”

Table = “your-table”

Sql_query = “SELECT column_name FROM `{}.{}.{}`”.format(project_id, dataset, table)



Df = spark.read \

    .format(“bigquery”) \

    .option(“query”, sql_query) \

    .load()



# Perform text cleaning, lemmatization, and stemming

Df_cleaned = df.withColumn(“cleaned_text”, clean_text_udf(col(“column_name”))) \

               .withColumn(“lemmatized_text”, lemmatize_text_udf(col(“cleaned_text”))) \

               .withColumn(“stemmed_text”, stem_text_udf(col(“cleaned_text”)))



# Show results

Df_cleaned.show(truncate=False)



# Save the processed data to a desired output location (e.g., back to BigQuery or a file)

Output_table = “your-output-table”

Df_cleaned.write \

    .format(“bigquery”) \

    .option(“table”, output_table) \

    .save()

%%writefile pyspark_bq_text_cleaning.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import nltk
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.corpus import stopwords
import re

# Download NLTK resources
nltk.download('wordnet')
nltk.download('stopwords')

# Initialize lemmatizer and stemmer
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

# Define text cleaning, lemmatization, and stemming functions
def clean_text(text):
    text = re.sub(r'\W', ' ', text)  # Remove special characters
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'\d', '', text)  # Remove digits
    text = text.strip().lower()  # Strip spaces and convert to lowercase
    return text

def lemmatize_text(text):
    return ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])

def stem_text(text):
    return ' '.join([stemmer.stem(word) for word in text.split() if word not in stop_words])

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("BigQueryTextProcessing") \
    .config("spark.jars", "path/to/spark-bigquery-connector.jar") \
    .getOrCreate()

# Read data from BigQuery using SQL query
project_id = "your-project-id"
dataset = "your-dataset"
table = "your-table"
sql_query = "SELECT column_name FROM `{}.{}.{}`".format(project_id, dataset, table)

df = spark.read \
    .format("bigquery") \
    .option("query", sql_query) \
    .load()

# Convert DataFrame to RDD for map operations
rdd = df.rdd.map(lambda row: row['column_name'])

# Apply text processing functions using map
rdd_cleaned = rdd.map(lambda text: clean_text(text))
rdd_lemmatized = rdd_cleaned.map(lambda text: lemmatize_text(text))
rdd_stemmed = rdd_cleaned.map(lambda text: stem_text(text))

# Convert back to DataFrame
df_processed = rdd_cleaned.zip(rdd_lemmatized).zip(rdd_stemmed).map(
    lambda x: (x[0], x[1][0], x[1][1])
).toDF(["cleaned_text", "lemmatized_text", "stemmed_text"])

# Show results
df_processed.show(truncate=False)

# Save the processed data to a desired output location (e.g., back to BigQuery or a file)
output_table = "your-output-table"
df_processed.write \
    .format("bigquery") \
    .option("table", output_table) \
    .save()

# Stop the SparkSession
spark.stop()

# Stop the SparkSession

Spark.stop()


import numpy as np

def sentence_to_vector(sentence, model):
    words = sentence.decode("utf-8").split()  # Tokenize the sentence
    word_vectors = [model[word] for word in words if word in model]
    if not word_vectors:  # If no words are in the model, return a zero vector


import pyarrow.parquet as pq
import numpy as np
import xgboost as xgb
from sklearn.metrics import mean_squared_error

# File paths
vectors_file = "vectors.parquet"
targets_file = "targets.parquet"

# Step 1: Load Parquet files in chunks
vectors_pq = pq.ParquetFile(vectors_file)
targets_pq = pq.ParquetFile(targets_file)

# Initialize arrays for training
batch_size = 10000  # Adjust as per memory availability
train_data = []
train_labels = []

# Step 2: Process row groups
for i in range(vectors_pq.num_row_groups):
    # Load a chunk of vectors and targets
    vectors_chunk = vectors_pq.read_row_group(i).to_pandas().to_dict()
    targets_chunk = targets_pq.read_row_group(i).to_pandas().to_dict()
    
    # Convert dictionaries to arrays
    for row_id, vector in vectors_chunk.items():
        train_data.append(vector)
        train_labels.append(targets_chunk[row_id])
    
    # If batch size reached, train XGBoost
    if len(train_data) >= batch_size:
        X = np.array(train_data, dtype=np.float32)
        y = np.array(train_labels, dtype=np.float32)
        dmatrix = xgb.DMatrix(X, label=y)
        
        # Train XGBoost model
        if i == 0:
            params = {
                "objective": "reg:squarederror",
                "max_depth": 6,
                "learning_rate": 0.1,
                "n_estimators": 100,
            }
            model = xgb.train(params, dmatrix, num_boost_round=10)
        else:
            model = xgb.train(params, dmatrix, num_boost_round=10, xgb_model=model)
        
        # Clear memory for next batch
        train_data, train_labels = [], []

# Step 3: Evaluate the model
if train_data:
    # Train with remaining data
    X = np.array(train_data, dtype=np.float32)
    y = np.array(train_labels, dtype=np.float32)
    dmatrix = xgb.DMatrix(X, label=y)
    model = xgb.train(params, dmatrix, num_boost_round=10, xgb_model=model)

# Save the model
model.save_model("xgboost_model.json")

print("Training complete and model saved!")

        return np.zeros(model.vector_size)
    return np.mean(word_vectors, axis=0)

# Generate vectors for all sentences
sentence_vectors = []
for sentence in parsed_sentences:
    vector = sentence_to_vector(sentence.numpy(), word2vec_model)
    sentence_vectors.append(vector)




