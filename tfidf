from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from scipy.sparse import vstack, csr_matrix
import joblib

# Define a function to load text data in batches
def load_text_data_in_batches(file_path, batch_size=1000):
    """
    Generator function to load text data in batches.
    Args:
        file_path (str): Path to the text file containing the data.
        batch_size (int): Number of lines to process in each batch.
    Yields:
        list: A batch of lines from the file.
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        batch = []
        for line in f:
            batch.append(line.strip())
            if len(batch) == batch_size:
                yield batch
                batch = []
        if batch:
            yield batch

# Function to align vocabularies and stack matrices
def align_and_stack(cumulative_counts, batch_counts, cumulative_vocab, batch_vocab):
    """
    Aligns the vocabularies of cumulative_counts and batch_counts and stacks them.
    Args:
        cumulative_counts: The cumulative sparse matrix.
        batch_counts: The current batch's sparse matrix.
        cumulative_vocab: The cumulative vocabulary (word to index mapping).
        batch_vocab: The batch's vocabulary (word to index mapping).
    Returns:
        Updated cumulative_counts and cumulative_vocab.
    """
    # Update cumulative_vocab with new words from batch_vocab
    offset = len(cumulative_vocab)
    for word, idx in batch_vocab.items():
        if word not in cumulative_vocab:
            cumulative_vocab[word] = offset
            offset += 1

    # Create aligned sparse matrices
    total_features = len(cumulative_vocab)
    if cumulative_counts is not None:
        cumulative_aligned = csr_matrix((cumulative_counts.shape[0], total_features), dtype=cumulative_counts.dtype)
        cumulative_aligned[:, :cumulative_counts.shape[1]] = cumulative_counts
    else:
        cumulative_aligned = csr_matrix((0, total_features), dtype=batch_counts.dtype)

    batch_aligned = csr_matrix((batch_counts.shape[0], total_features), dtype=batch_counts.dtype)
    for word, idx in batch_vocab.items():
        aligned_idx = cumulative_vocab[word]
        batch_aligned[:, aligned_idx] = batch_counts[:, idx]

    # Stack the aligned matrices
    cumulative_counts = vstack([cumulative_aligned, batch_aligned])
    return cumulative_counts, cumulative_vocab

# File path and batch size
file_path = "large_text_data.txt"  # Replace with your file path
batch_size = 1000

# Initialize vectorizer, transformer, and placeholders
count_vectorizer = CountVectorizer()
tfidf_transformer = TfidfTransformer()
cumulative_counts = None
cumulative_vocab = {}

# Process data in batches
for batch in load_text_data_in_batches(file_path, batch_size=batch_size):
    print(f"Processing a batch of size {len(batch)}...")

    # Generate counts for the batch and get the batch vocabulary
    batch_counts = count_vectorizer.fit_transform(batch)
    batch_vocab = count_vectorizer.vocabulary_

    # Align and stack the batch with the cumulative data
    cumulative_counts, cumulative_vocab = align_and_stack(cumulative_counts, batch_counts, cumulative_vocab, batch_vocab)

# Compute TF-IDF on the cumulative counts
print("Computing TF-IDF...")
tfidf_matrix = tfidf_transformer.fit_transform(cumulative_counts)

# Save the models and the final matrix
joblib.dump(cumulative_vocab, 'cumulative_vocab.pkl')
joblib.dump(tfidf_transformer, 'tfidf_transformer.pkl')
joblib.dump(tfidf_matrix, 'tfidf_matrix.pkl')

print("Processing complete. Models and matrices saved.")
